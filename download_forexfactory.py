import time
import random
import datetime
import gspread
import pandas as pd
from oauth2client.service_account import ServiceAccountCredentials
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import undetected_chromedriver as uc

# =========================
# Config
# =========================
CREDENTIALS_FILE = "credentials.json"
SPREADSHEET_ID = "1ayYyGCF6RLaiizDqLRdU7EN7rwhQDbkMzG3IssMJgBM"   # ‚ö†Ô∏è Mets ton vrai ID ici
WORKSHEET_NAME = "forex"  # Onglet Google Sheets

FOREXFACTORY_URL = "https://www.forexfactory.com/calendar?week={}"

# =========================
# Scraping
# =========================
def scrape_forex_factory(week):
    url = FOREXFACTORY_URL.format(week)
    print(f"üåê Scraping URL : {url}")

    options = uc.ChromeOptions()
    options.add_argument("--headless=new")
    driver = uc.Chrome(options=options)

    try:
        driver.get(url)
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "tr.calendar__row")))

        rows = driver.find_elements(By.CSS_SELECTOR, "tr.calendar__row")

        events = []
        current_date = None

        for row in rows:
            try:
                # V√©rifier la date
                date_cell = row.find_element(By.CSS_SELECTOR, "td.calendar__date").text.strip()
                if date_cell:
                    current_date = date_cell
                if not current_date:
                    continue

                impact = row.find_element(By.CSS_SELECTOR, "td.calendar__impact span").get_attribute("title")

                # Mapper impact ‚Üí nombre de taureaux
                impact_map = {
                    "Low Impact Expected": "1 t√™te",
                    "Medium Impact Expected": "2 t√™tes",
                    "High Impact Expected": "3 t√™tes"
                }
                impact_label = impact_map.get(impact, impact)

                if "Impact" not in impact:
                    continue  # √©viter les lignes vides

                time_cell = row.find_element(By.CSS_SELECTOR, "td.calendar__time").text.strip()
                currency = row.find_element(By.CSS_SELECTOR, "td.calendar__currency").text.strip()
                event = row.find_element(By.CSS_SELECTOR, "td.calendar__event").text.strip()
                forecast = row.find_element(By.CSS_SELECTOR, "td.calendar__forecast").text.strip()
                previous = row.find_element(By.CSS_SELECTOR, "td.calendar__previous").text.strip()
                actual = row.find_element(By.CSS_SELECTOR, "td.calendar__actual").text.strip()

                events.append({
                    "Date": current_date,
                    "Heure": time_cell,
                    "Devise": currency,
                    "√âv√©nement": event,
                    "Impact": impact_label,
                    "Pr√©vision": forecast,
                    "Pr√©c√©dent": previous,
                    "R√©el": actual
                })
            except Exception:
                continue

        df = pd.DataFrame(events)
        print(f"‚úÖ {len(df)} √©v√©nements r√©cup√©r√©s ({week})")
        return df

    except Exception as e:
        print(f"‚ùå Erreur scraping : {e}")
        return pd.DataFrame()

    finally:
        driver.quit()

# =========================
# Google Sheets
# =========================
def update_google_sheets(df):
    if df.empty:
        print("‚ö†Ô∏è Aucun √©v√©nement √† envoyer dans Google Sheets.")
        return
    try:
        print("üì° Connexion √† Google Sheets‚Ä¶")
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        creds = ServiceAccountCredentials.from_json_keyfile_name(CREDENTIALS_FILE, scope)
        client = gspread.authorize(creds)
        ws = client.open_by_key(SPREADSHEET_ID).worksheet(WORKSHEET_NAME)
        ws.clear()
        ws.update([df.columns.tolist()] + df.values.tolist())
        print("‚úÖ Donn√©es mises √† jour avec succ√®s !")
    except Exception as e:
        print(f"‚ùå Erreur Google Sheets : {e}")

# =========================
# V√©rification semaine courante/next
# =========================
def get_next_week_if_needed(df, current_week):
    if df.empty:
        print("‚ö†Ô∏è Aucun √©v√©nement trouv√© cette semaine, on passe √† la suivante.")
        return current_week + datetime.timedelta(weeks=1)

    try:
        last_event_date = df["Date"].dropna().iloc[-1]
        print(f"üìÖ Dernier √©v√©nement trouv√© : {last_event_date}")
        today = datetime.date.today().strftime("%b %d")
        if today > last_event_date:
            print("‚û°Ô∏è La semaine est termin√©e, on scrape la suivante.")
            return current_week + datetime.timedelta(weeks=1)
        else:
            print("‚úÖ La semaine en cours contient encore des √©v√©nements.")
            return current_week
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la v√©rification de la semaine : {e}")
        return current_week

# =========================
# Main
# =========================
def main():
    today = datetime.date.today()
    monday = today - datetime.timedelta(days=today.weekday())  # lundi de la semaine
    current_week = monday

    df = scrape_forex_factory(current_week)
    week_to_scrape = get_next_week_if_needed(df, current_week)

    if week_to_scrape != current_week:
        time.sleep(random.randint(20, 40))  # pause pour √©viter blocage
        df = scrape_forex_factory(week_to_scrape)

    update_google_sheets(df)

if __name__ == "__main__":
    main()
